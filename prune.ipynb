{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from utils import *\n",
    "from prune import *\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jocl/.conda/envs/thesis/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/jocl/.conda/envs/thesis/lib/python3.10/site-packages/torch/cuda/__init__.py:758: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bigscience/bloom-560m\", output_attentions=True)\n",
    "n_layer, n_head = get_model_layers_and_heads(model.config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model.eval()\n",
    "\n",
    "prompts = get_prompts_from_file(\"paws_en\")\n",
    "\n",
    "prune_percent = 0.25\n",
    "n_groups = (n_head * n_layer) - int(n_layer * n_head * prune_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head*n_layer - n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(n_head*n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# clustering_dict, attentions, attention_vectors = get_clustering_dict(prompts, model, tokenizer, n_groups, \"cosine\",  n_layer, n_head, by_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering\n",
      "Clustering Done\n",
      "Counter({1: 244, 2: 23, 3: 8, 4: 7, 6: 2, 9: 2, 7: 1, 5: 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/data_shares/mapillary/thesis_models/pruned_models/bloom-560m/imbalanced/paws_en/euclidean_euclidean/0.25'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/data_shares/mapillary/thesis_models/pruned_models/\"\n",
    "duplicate_prune_model(prompts,path,model, model_name=\"bigscience/bloom-560m\", tokenizer=tokenizer, prune_method=\"imbalanced\", prune_task=\"paws_en\", prune_percent=0.25, metric=\"euclidean\",group_metric=\"euclidean\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metric = 'cosine'\n",
    "if group_metric != 'random':\n",
    "    squaref = squareform(pdist(attention_vectors, metric=group_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of groups:  Counter({1: 257, 2: 12, 3: 7, 4: 6, 6: 3, 12: 1, 23: 1, 5: 1})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "pruning_log = []\n",
    "verbose = True\n",
    "for group in clustering_dict.values():\n",
    "    counter.update([len(group)])\n",
    "    group_scores = defaultdict(int)\n",
    "    if len(group) <= 1:\n",
    "        continue\n",
    "    if len(group) == 2:\n",
    "        # with 2 heads just keep the first 1\n",
    "        head_to_keep = group[0]\n",
    "    else:\n",
    "        if group_metric == 'random':\n",
    "            head_to_keep = random.choice(group)\n",
    "        else:\n",
    "            for head_id in group:\n",
    "                for head_id_2 in group:\n",
    "                    if head_id == head_id_2:\n",
    "                        continue\n",
    "                    head1 = head_id[0]*n_head + head_id[1]\n",
    "                    head2 = head_id_2[0]*n_head + head_id_2[1]\n",
    "                    group_scores[head_id] += squaref[head1, head2]\n",
    "            head_to_keep = min(group_scores, key=lambda k: group_scores[k])\n",
    "            \n",
    "    for head in group:\n",
    "        if head == head_to_keep:\n",
    "            continue\n",
    "        head_to_remove = head\n",
    "        pruning_log.append((head_to_keep, head_to_remove))\n",
    "        model = duplicate_prune(model, source_layer=head_to_keep[0], source_head=head_to_keep[1], target_layer=head_to_remove[0], target_head=head_to_remove[1])\n",
    "if verbose:\n",
    "    print(\"size of groups: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pruning_log[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = []\n",
    "for head in range(n_head):\n",
    "    for layer in range(n_layer):\n",
    "        heads.append((layer, head))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0.5*n_head*n_layer\n",
    "to_prune = random.choices(heads, k=int(k))\n",
    "pruning_dict = defaultdict(list)\n",
    "for head in to_prune:\n",
    "    pruning_dict[head[0]].append(head[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 5),\n",
       " (18, 15),\n",
       " (2, 9),\n",
       " (11, 14),\n",
       " (4, 12),\n",
       " (0, 5),\n",
       " (23, 5),\n",
       " (12, 3),\n",
       " (13, 12),\n",
       " (8, 6),\n",
       " (2, 6),\n",
       " (17, 3),\n",
       " (23, 1),\n",
       " (12, 12),\n",
       " (9, 14),\n",
       " (23, 15),\n",
       " (18, 6),\n",
       " (2, 3),\n",
       " (11, 3),\n",
       " (0, 1),\n",
       " (3, 3),\n",
       " (2, 1),\n",
       " (17, 3),\n",
       " (8, 6),\n",
       " (8, 3),\n",
       " (5, 0),\n",
       " (4, 10),\n",
       " (20, 9),\n",
       " (3, 14),\n",
       " (15, 2),\n",
       " (17, 1),\n",
       " (0, 2),\n",
       " (4, 14),\n",
       " (21, 10),\n",
       " (2, 1),\n",
       " (14, 10),\n",
       " (18, 14),\n",
       " (15, 15),\n",
       " (21, 10),\n",
       " (19, 4),\n",
       " (15, 1),\n",
       " (7, 2),\n",
       " (8, 2),\n",
       " (16, 12),\n",
       " (18, 3),\n",
       " (3, 6),\n",
       " (22, 5),\n",
       " (23, 10),\n",
       " (14, 11),\n",
       " (10, 11),\n",
       " (22, 4),\n",
       " (8, 0),\n",
       " (9, 11),\n",
       " (8, 11),\n",
       " (22, 11),\n",
       " (5, 14),\n",
       " (3, 1),\n",
       " (14, 14),\n",
       " (0, 5),\n",
       " (2, 9),\n",
       " (9, 10),\n",
       " (13, 15),\n",
       " (19, 7),\n",
       " (22, 5),\n",
       " (5, 6),\n",
       " (18, 7),\n",
       " (0, 3),\n",
       " (3, 1),\n",
       " (22, 14),\n",
       " (1, 7),\n",
       " (9, 8),\n",
       " (9, 4),\n",
       " (14, 1),\n",
       " (7, 6),\n",
       " (14, 15),\n",
       " (10, 12),\n",
       " (17, 13),\n",
       " (0, 8),\n",
       " (16, 15),\n",
       " (5, 10),\n",
       " (6, 11),\n",
       " (7, 4),\n",
       " (18, 1),\n",
       " (5, 8),\n",
       " (21, 5),\n",
       " (19, 11),\n",
       " (7, 13),\n",
       " (8, 5),\n",
       " (8, 10),\n",
       " (5, 7),\n",
       " (21, 8),\n",
       " (19, 8),\n",
       " (14, 12),\n",
       " (21, 14),\n",
       " (21, 15),\n",
       " (0, 6),\n",
       " (9, 10),\n",
       " (22, 8),\n",
       " (23, 12),\n",
       " (16, 8),\n",
       " (9, 11),\n",
       " (18, 6),\n",
       " (19, 4),\n",
       " (11, 12),\n",
       " (1, 10),\n",
       " (16, 15),\n",
       " (9, 7),\n",
       " (7, 4),\n",
       " (16, 12),\n",
       " (10, 5),\n",
       " (11, 6),\n",
       " (0, 9),\n",
       " (16, 10),\n",
       " (14, 14),\n",
       " (6, 3),\n",
       " (2, 12),\n",
       " (18, 6),\n",
       " (15, 11),\n",
       " (9, 15),\n",
       " (17, 10),\n",
       " (7, 5),\n",
       " (8, 6),\n",
       " (5, 4),\n",
       " (3, 5),\n",
       " (18, 12),\n",
       " (23, 0),\n",
       " (9, 1),\n",
       " (22, 3),\n",
       " (21, 3),\n",
       " (10, 4),\n",
       " (19, 2),\n",
       " (20, 8),\n",
       " (20, 11),\n",
       " (1, 2),\n",
       " (22, 0),\n",
       " (23, 8),\n",
       " (23, 5),\n",
       " (16, 7),\n",
       " (22, 6),\n",
       " (15, 15),\n",
       " (19, 1),\n",
       " (20, 12),\n",
       " (23, 0),\n",
       " (22, 11),\n",
       " (10, 14),\n",
       " (22, 3),\n",
       " (9, 0),\n",
       " (20, 1),\n",
       " (10, 5),\n",
       " (12, 5),\n",
       " (12, 13),\n",
       " (4, 10),\n",
       " (16, 8),\n",
       " (13, 15),\n",
       " (18, 2),\n",
       " (7, 10),\n",
       " (18, 5),\n",
       " (5, 10),\n",
       " (14, 8),\n",
       " (22, 11),\n",
       " (22, 15),\n",
       " (5, 10),\n",
       " (7, 1),\n",
       " (23, 7),\n",
       " (20, 13),\n",
       " (17, 15),\n",
       " (20, 3),\n",
       " (1, 10),\n",
       " (8, 15),\n",
       " (10, 0),\n",
       " (17, 1),\n",
       " (2, 6),\n",
       " (23, 5),\n",
       " (4, 4),\n",
       " (5, 11),\n",
       " (22, 11),\n",
       " (11, 1),\n",
       " (8, 10),\n",
       " (17, 15),\n",
       " (9, 2),\n",
       " (8, 8),\n",
       " (11, 0),\n",
       " (11, 12),\n",
       " (1, 12),\n",
       " (20, 9),\n",
       " (3, 1),\n",
       " (6, 9),\n",
       " (9, 5),\n",
       " (12, 12),\n",
       " (13, 4),\n",
       " (17, 7),\n",
       " (14, 13)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_prune"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
